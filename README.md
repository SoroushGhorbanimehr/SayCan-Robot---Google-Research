# SayCan-Robot---Google-Research
Grounding Language in Robotic Affordances


## Overview
Large language models encode extensive semantic knowledge, which could be highly beneficial for robots tasked with executing high-level, complex instructions given in natural language. However, the challenge lies in the fact that language models lack real-world experience, making it difficult to apply their knowledge directly to decision-making within a physical embodiment. For instance, while a language model might describe how to clean a spill effectively, it might not account for the specific capabilities and constraints of a robot tasked with performing this action.

SayCan addresses this gap by integrating real-world grounding through pretrained robotic skills. These skills act as constraints, guiding the language model to propose feasible and contextually appropriate actions. In this framework, the robot functions as the language model's "hands and eyes," executing tasks based on the high-level semantic guidance provided by the model.

This project demonstrates how low-level skills can be combined with large language models to accomplish complex and temporally extended instructions. The method is evaluated on various real-world robotic tasks, highlighting the necessity of real-world grounding and the effectiveness of this approach in completing long-horizon, abstract instructions using a mobile manipulator.

## Getting Started
This repository contains a self-contained implementation of SayCan for a table-top environment. Follow the instructions below to get started.

## Installation
1- Clone the repository:
** git clone https://github.com/SoroushGhorbanimehr/SayCan-Robot---Google-Research.git **
** cd DIR (Will be updated) ** 

# Will be updated





